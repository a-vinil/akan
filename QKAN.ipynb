{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5b5fc-8057-4b4e-84e0-198df77b5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pennylane as qml\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "# KANLinear definition Soure: https://github.com/Blealtan/efficient-kan/blob/f39e5146af34299ad3a581d2106eb667ba0fa6fa/src/efficient_kan/kan.py#L6\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "# Quantum layer using PennyLane\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, n_features):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Define a Pennylane device\n",
    "        self.device = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "        # Define a simple quantum circuit\n",
    "        def circuit(inputs, weights):\n",
    "            for i in range(len(inputs)):  # Use len(inputs) to avoid IndexError\n",
    "                qml.RX(inputs[i], wires=i)\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "        \n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "        # Create a quantum node\n",
    "        weight_shapes = {\"weights\": (3, n_qubits)}  # Example: 3 layers of entanglement\n",
    "        self.qnode = qml.QNode(circuit, self.device, interface=\"torch\")\n",
    "\n",
    "        # Convert the quantum node to a torch layer\n",
    "        self.qlayer = qml.qnn.TorchLayer(self.qnode, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input size matches the number of qubits\n",
    "        if x.size(1) < self.n_qubits:\n",
    "            raise ValueError(\n",
    "                f\"Input features ({x.size(1)}) must be at least the number of qubits ({self.n_qubits}).\"\n",
    "            )\n",
    "        x = x[:, :self.n_qubits]  # Use only the required features\n",
    "        return self.qlayer(x)\n",
    "\n",
    "\n",
    "# CNN model with KAN and Quantum Layer\n",
    "class CNNKANQuantum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNKANQuantum, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.kan1 = KANLinear(64 * 8 * 8, 256)\n",
    "        self.quantum_layer = QuantumLayer(n_qubits=8, n_features=256)\n",
    "        self.kan2 = KANLinear(8, 10)  # Adjust output size to match quantum layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.selu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.kan1(x)\n",
    "        x = self.quantum_layer(x)\n",
    "        x = self.kan2(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNKANQuantum().to(device)\n",
    "print(model)\n",
    "\n",
    "# Print parameter details\n",
    "def print_parameter_details(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            params = parameter.numel()\n",
    "            total_params += params\n",
    "            print(f\"{name}: {params}\")\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "print_parameter_details(model)\n",
    "summary(model, input_size=(3, 32, 32))\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss(reduction='sum')(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cd93bf-a97a-49b9-96e3-7bd227095e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNKANWithQuantum(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (kan1): KANLinear(\n",
      "    (base_activation): SiLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      "  (quantum_layers): ModuleList(\n",
      "    (0-1): 2 x QuantumLayer()\n",
      "  )\n",
      "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
      ")\n",
      "conv1.weight: 864\n",
      "conv1.bias: 32\n",
      "conv2.weight: 18432\n",
      "conv2.bias: 64\n",
      "kan1.base_weight: 1048576\n",
      "kan1.spline_weight: 8388608\n",
      "kan1.spline_scaler: 1048576\n",
      "fc.weight: 1024\n",
      "fc.bias: 4\n",
      "quantum_layers.0.weights: 24\n",
      "quantum_layers.1.weights: 24\n",
      "output.weight: 40\n",
      "output.bias: 10\n",
      "Total trainable parameters: 10506278\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "         MaxPool2d-2           [-1, 32, 16, 16]               0\n",
      "            Conv2d-3           [-1, 64, 16, 16]          18,496\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "              SiLU-5                 [-1, 4096]               0\n",
      "         KANLinear-6                  [-1, 256]               0\n",
      "            Linear-7                    [-1, 4]           1,028\n",
      "      QuantumLayer-8                    [-1, 4]               0\n",
      "      QuantumLayer-9                    [-1, 4]               0\n",
      "           Linear-10                   [-1, 10]              50\n",
      "================================================================\n",
      "Total params: 20,470\n",
      "Trainable params: 20,470\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.50\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.59\n",
      "----------------------------------------------------------------\n",
      "conv1.weight: torch.Size([32, 3, 3, 3]) requires_grad\n",
      "conv1.bias: torch.Size([32]) requires_grad\n",
      "conv2.weight: torch.Size([64, 32, 3, 3]) requires_grad\n",
      "conv2.bias: torch.Size([64]) requires_grad\n",
      "kan1.base_weight: torch.Size([256, 4096]) requires_grad\n",
      "kan1.spline_weight: torch.Size([256, 4096, 8]) requires_grad\n",
      "kan1.spline_scaler: torch.Size([256, 4096]) requires_grad\n",
      "fc.weight: torch.Size([4, 256]) requires_grad\n",
      "fc.bias: torch.Size([4]) requires_grad\n",
      "quantum_layers.0.weights: torch.Size([2, 4, 3]) requires_grad\n",
      "quantum_layers.1.weights: torch.Size([2, 4, 3]) requires_grad\n",
      "output.weight: torch.Size([10, 4]) requires_grad\n",
      "output.bias: torch.Size([10]) requires_grad\n",
      "Train Epoch: 0 [0/12049 (0%)]\tLoss: 2.208966\n",
      "Train Epoch: 0 [5000/12049 (40%)]\tLoss: 2.166083\n",
      "Train Epoch: 0 [10000/12049 (80%)]\tLoss: 2.123529\n",
      "\n",
      "Test set: Average loss: 0.0086, Accuracy: 0/1968 (0%)\n",
      "\n",
      "Train Epoch: 1 [0/12049 (0%)]\tLoss: 2.110783\n",
      "Train Epoch: 1 [5000/12049 (40%)]\tLoss: 2.070982\n",
      "Train Epoch: 1 [10000/12049 (80%)]\tLoss: 2.031351\n",
      "\n",
      "Test set: Average loss: 0.0082, Accuracy: 0/1968 (0%)\n",
      "\n",
      "Train Epoch: 2 [0/12049 (0%)]\tLoss: 2.015664\n",
      "Train Epoch: 2 [5000/12049 (40%)]\tLoss: 1.979327\n",
      "Train Epoch: 2 [10000/12049 (80%)]\tLoss: 1.941198\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 873/1968 (44%)\n",
      "\n",
      "Train Epoch: 3 [0/12049 (0%)]\tLoss: 1.926669\n",
      "Train Epoch: 3 [5000/12049 (40%)]\tLoss: 1.890586\n",
      "Train Epoch: 3 [10000/12049 (80%)]\tLoss: 1.855842\n",
      "\n",
      "Test set: Average loss: 0.0075, Accuracy: 958/1968 (49%)\n",
      "\n",
      "Train Epoch: 4 [0/12049 (0%)]\tLoss: 1.842998\n",
      "Train Epoch: 4 [5000/12049 (40%)]\tLoss: 1.810515\n",
      "Train Epoch: 4 [10000/12049 (80%)]\tLoss: 1.776266\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy: 958/1968 (49%)\n",
      "\n",
      "Train Epoch: 5 [0/12049 (0%)]\tLoss: 1.764025\n",
      "Train Epoch: 5 [5000/12049 (40%)]\tLoss: 1.733894\n",
      "Train Epoch: 5 [10000/12049 (80%)]\tLoss: 1.703320\n",
      "\n",
      "Test set: Average loss: 0.0069, Accuracy: 958/1968 (49%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from torch.nn import Module\n",
    "\n",
    "# KANLinear definition Soure: https://github.com/Blealtan/efficient-kan/blob/f39e5146af34299ad3a581d2106eb667ba0fa6fa/src/efficient_kan/kan.py#L6\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# class QuantumLayer(nn.Module):\n",
    "#     def __init__(self, n_qubits, n_layers, seed=None):\n",
    "#         super(QuantumLayer, self).__init__()\n",
    "        \n",
    "#         # Random seed for reproducibility if provided\n",
    "#         if seed is not None:\n",
    "#             torch.manual_seed(seed)\n",
    "#             np.random.seed(seed)\n",
    "\n",
    "#         self.n_qubits = n_qubits\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         # Quantum device initialization\n",
    "#         self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "\n",
    "#         # Define the quantum circuit\n",
    "#         @qml.qnode(self.dev, interface=\"torch\")\n",
    "#         def circuit(inputs, weights):\n",
    "#             # Encode classical data\n",
    "#             for i in range(self.n_qubits):\n",
    "#                 qml.RX(inputs[i], wires=i)\n",
    "\n",
    "#             # Variational layer with entangling gates\n",
    "#             qml.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
    "\n",
    "#             # Measurement of all qubits using PauliZ\n",
    "#             return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "#         self.circuit = circuit\n",
    "\n",
    "#         # Initialize quantum weights with He initialization for stability\n",
    "#         self.weights = nn.Parameter(torch.randn(self.n_layers, self.n_qubits, 3, dtype=torch.float32) * np.sqrt(2. / self.n_qubits))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Flatten the input for quantum encoding\n",
    "#         batch_size = x.size(0)\n",
    "#         x = x.view(batch_size, -1)\n",
    "        \n",
    "#         results = []\n",
    "    \n",
    "#         for i in range(batch_size):\n",
    "#             # Normalize inputs to [0, π], ensuring that the data fits the quantum circuit expectations\n",
    "#             inputs = x[i, :self.n_qubits]  \n",
    "#             inputs = torch.tanh(inputs) * np.pi  # Normalizing inputs within [0, π]\n",
    "            \n",
    "#             # Convert quantum circuit output to torch.Tensor and match device\n",
    "#             circuit_output = torch.tensor(\n",
    "#                 self.circuit(inputs, self.weights), dtype=torch.float32\n",
    "#             ).to(x.device)\n",
    "            \n",
    "#             # Post-processing: apply activation function (like tanh) to quantum outputs\n",
    "#             results.append(torch.tanh(circuit_output))  # Activation to regularize output\n",
    "    \n",
    "#         return torch.stack(results)\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\"Enhanced Quantum Layer with improved architecture and error handling\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_qubits: int, \n",
    "        n_layers: int,\n",
    "        activation: str = 'tanh',\n",
    "        device_type: str = 'default.qubit',\n",
    "        seed: Optional[int] = None,\n",
    "        noise_strength: float = 0.0\n",
    "    ):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        \n",
    "        if n_qubits <= 0 or n_layers <= 0:\n",
    "            raise ValueError(\"n_qubits and n_layers must be positive integers\")\n",
    "            \n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.noise_strength = noise_strength\n",
    "        \n",
    "        # Set random seeds if provided\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Activation function selection\n",
    "        self.activation = {\n",
    "            'tanh': torch.tanh,\n",
    "            'relu': F.relu,\n",
    "            'sigmoid': torch.sigmoid\n",
    "        }.get(activation, torch.tanh)\n",
    "        \n",
    "        # Initialize quantum device with error handling\n",
    "        try:\n",
    "            self.dev = qml.device(device_type, wires=self.n_qubits)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize quantum device: {str(e)}\")\n",
    "            \n",
    "        # Define quantum circuit with improved architecture\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "        def circuit(inputs: torch.Tensor, weights: torch.Tensor) -> List[float]:\n",
    "            # Input encoding layer\n",
    "            self._encode_inputs(inputs)\n",
    "            \n",
    "            # Variational layers\n",
    "            self._apply_variational_layers(weights)\n",
    "            \n",
    "            # Add noise if specified (for robustness training)\n",
    "            if self.noise_strength > 0:\n",
    "                self._apply_noise()\n",
    "            \n",
    "            # Measurement layer\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "            \n",
    "        self.circuit = circuit\n",
    "        \n",
    "        # Initialize weights with improved initialization scheme\n",
    "        weight_shape = (self.n_layers, self.n_qubits, 3)\n",
    "        self.weights = nn.Parameter(\n",
    "            self._initialize_weights(weight_shape)\n",
    "        )\n",
    "        \n",
    "    def _initialize_weights(self, shape: Tuple) -> torch.Tensor:\n",
    "        \"\"\"Improved weight initialization with scale adjustment\"\"\"\n",
    "        scale = np.sqrt(2.0 / (self.n_qubits * self.n_layers))\n",
    "        return torch.randn(shape, dtype=torch.float32) * scale\n",
    "        \n",
    "    def _encode_inputs(self, inputs: torch.Tensor) -> None:\n",
    "        \"\"\"Enhanced input encoding with amplitude encoding\"\"\"\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "            qml.RZ(inputs[i], wires=i)\n",
    "            \n",
    "    def _apply_variational_layers(self, weights: torch.Tensor) -> None:\n",
    "        \"\"\"Apply variational layers with improved entanglement\"\"\"\n",
    "        for layer in range(self.n_layers):\n",
    "            # Custom entangling layers\n",
    "            qml.StronglyEntanglingLayers(\n",
    "                weights[layer].reshape(1, self.n_qubits, 3), \n",
    "                wires=range(self.n_qubits)\n",
    "            )\n",
    "            # Add CNOT ladder for better entanglement\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "                \n",
    "    def _apply_noise(self) -> None:\n",
    "        \"\"\"Apply controlled noise for robustness\"\"\"\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.DepolarizingChannel(self.noise_strength, wires=i)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                # Improved input preprocessing\n",
    "                inputs = x[i, :self.n_qubits]\n",
    "                inputs = self._preprocess_inputs(inputs)\n",
    "                \n",
    "                # Execute quantum circuit\n",
    "                circuit_output = torch.tensor(\n",
    "                    self.circuit(inputs, self.weights),\n",
    "                    dtype=torch.float32\n",
    "                ).to(x.device)\n",
    "                \n",
    "                # Post-process output\n",
    "                processed_output = self._postprocess_output(circuit_output)\n",
    "                results.append(processed_output)\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error in quantum circuit execution: {str(e)}\")\n",
    "                \n",
    "        return torch.stack(results)\n",
    "        \n",
    "    def _preprocess_inputs(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Enhanced input preprocessing with normalization\"\"\"\n",
    "        # Scale inputs to [-π/2, π/2] for better quantum encoding\n",
    "        return torch.arctan(inputs) * 2\n",
    "        \n",
    "    def _postprocess_output(self, output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Enhanced output post-processing\"\"\"\n",
    "        return self.activation(output)\n",
    "\n",
    "# class CNNKANWithQuantum(nn.Module):\n",
    "#     def __init__(self, n_qubits=8, n_layers=1):\n",
    "#         super(CNNKANWithQuantum, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.kan1 = KANLinear(64 * 8 * 8, 256)\n",
    "#         self.quantum = QuantumLayer(n_qubits=n_qubits, n_layers=n_layers)\n",
    "#         self.fc = nn.Linear(n_qubits, 10)  # Map quantum outputs to class predictions\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.selu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.selu(self.conv2(x))\n",
    "#         x = self.pool2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.kan1(x)\n",
    "#         x = self.quantum(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "class CNNKANWithQuantum(nn.Module):\n",
    "    def __init__(self, n_qubits=4, n_layers=2):\n",
    "        super(CNNKANWithQuantum, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # For 3-channel input\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.kan1 = KANLinear(64 * 8 * 8, 256)  # Update input size for KANLinear\n",
    "        self.fc = nn.Linear(256, n_qubits)\n",
    "        # self.quantum = QuantumLayer(n_qubits=n_qubits, n_layers=n_layers)\n",
    "        self.quantum_layers = nn.ModuleList([QuantumLayer(n_qubits=n_qubits, n_layers=n_layers) for _ in range(n_layers)])\n",
    "\n",
    "        self.output = nn.Linear(n_qubits, 10)  # Map quantum outputs to class predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.selu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature map\n",
    "        x = self.kan1(x)\n",
    "        x = F.selu(self.fc(x))  # Apply fully connected layer\n",
    "        # x = self.quantum(x\n",
    "        for quantum_layer in self.quantum_layers:\n",
    "            x = quantum_layer(x)\n",
    "        x = self.output(x)  # Final classification layer\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.selu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.selu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flattening the layer for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.selu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def print_parameter_details(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            params = parameter.numel()  # Number of elements in the tensor\n",
    "            total_params += params\n",
    "            print(f\"{name}: {params}\")\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CNN().to(device)\n",
    "\n",
    "# Uncommnet this line for CNN KAN.\n",
    "model = CNNKANWithQuantum().to(device)\n",
    "print(model)\n",
    "print_parameter_details(model)\n",
    "summary(model,  input_size=(3, 32, 32))\n",
    "\n",
    "# Note the this is just a rough demo for Visualization. Need modifcation.\n",
    "def visualize_kan_parameters(kan_layer, layer_name):\n",
    "    base_weights = kan_layer.base_weight.data.cpu().numpy()\n",
    "    plt.hist(base_weights.ravel(), bins=50)\n",
    "    plt.title(f\"Distribution of Base Weights - {layer_name}\")\n",
    "    plt.xlabel(\"Weight Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    if hasattr(kan_layer, 'spline_weight'):\n",
    "        spline_weights = kan_layer.spline_weight.data.cpu().numpy()\n",
    "        plt.hist(spline_weights.ravel(), bins=50)\n",
    "        plt.title(f\"Distribution of Spline Weights - {layer_name}\")\n",
    "        plt.xlabel(\"Weight Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()} {'requires_grad' if param.requires_grad else 'frozen'}\")\n",
    "\n",
    "# TODO: Need to explore various Optimizer and optimize the Learning Rate.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "# ])\n",
    "# train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# # Transformations to reshape MNIST to (3, 32, 32)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),  # Resize to 32x32\n",
    "#     transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for 3 channels\n",
    "# ])\n",
    "\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# # Custom Dataset Wrapper to Filter Specific Labels\n",
    "# class FilteredMNIST(Dataset):\n",
    "#     def __init__(self, dataset, labels_to_include):\n",
    "#         \"\"\"\n",
    "#         Filters the given dataset to include only specified labels.\n",
    "        \n",
    "#         Args:\n",
    "#             dataset (Dataset): The dataset to filter.\n",
    "#             labels_to_include (list): List of labels to include in the filtered dataset.\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.labels_to_include = labels_to_include\n",
    "#         self.filtered_indices = [\n",
    "#             idx for idx, (_, label) in enumerate(dataset) if label in labels_to_include\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.filtered_indices)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         original_idx = self.filtered_indices[idx]\n",
    "#         return self.dataset[original_idx]\n",
    "\n",
    "# # # Updated Dataset and Dataloader\n",
    "# # train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# # test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# # Load the MNIST dataset\n",
    "# original_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# original_test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# # Filter the datasets for labels 3 and 6\n",
    "# filtered_train_dataset = FilteredMNIST(original_train_dataset, labels_to_include=[3, 6])\n",
    "# filtered_test_dataset = FilteredMNIST(original_test_dataset, labels_to_include=[3, 6])\n",
    "\n",
    "# # Create DataLoaders for the filtered datasets\n",
    "# train_loader = DataLoader(filtered_train_dataset, batch_size=500, shuffle=True)\n",
    "# test_loader = DataLoader(filtered_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "###############\n",
    "# Quantum Data Augmentation Class\n",
    "class QuantumDataAugmentation(nn.Module):\n",
    "    def __init__(self, n_qubits):\n",
    "        super(QuantumDataAugmentation, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "    def apply_random_quantum_rotation(self, image):\n",
    "        \"\"\"Apply random quantum rotations to the input image.\"\"\"\n",
    "        # Random angle for quantum rotation\n",
    "        theta = random.uniform(0, 2 * np.pi)\n",
    "        \n",
    "        # We will use the quantum rotation matrix for a single qubit (simplified)\n",
    "        # The rotation matrix in 1D (using a simple rotation in the quantum space)\n",
    "        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                                    [np.sin(theta), np.cos(theta)]])\n",
    "        \n",
    "        # Apply the rotation to the image by multiplying each pixel by a random angle (simulating rotation)\n",
    "        augmented_image = image * torch.tensor(rotation_matrix[0, 0], dtype=torch.float32)\n",
    "        \n",
    "        return augmented_image\n",
    "    \n",
    "    def forward(self, x):\n",
    "        augmented_images = []\n",
    "        for image in x:\n",
    "            augmented_images.append(self.apply_random_quantum_rotation(image))\n",
    "        \n",
    "        return torch.stack(augmented_images)\n",
    "\n",
    "# Define Transform for Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "# Custom Dataset Wrapper to Filter Specific Labels\n",
    "class FilteredMNIST(Dataset):\n",
    "    def __init__(self, dataset, labels_to_include, quantum_aug):\n",
    "        \"\"\"\n",
    "        Filters the given dataset to include only specified labels and applies quantum data augmentation.\n",
    "        \n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to filter.\n",
    "            labels_to_include (list): List of labels to include in the filtered dataset.\n",
    "            quantum_aug (QuantumDataAugmentation): Quantum data augmentation module to apply to the data.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels_to_include = labels_to_include\n",
    "        self.quantum_aug = quantum_aug\n",
    "        self.filtered_indices = [\n",
    "            idx for idx, (_, label) in enumerate(dataset) if label in labels_to_include\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.filtered_indices[idx]\n",
    "        image, label = self.dataset[original_idx]\n",
    "        \n",
    "        # Apply Quantum Data Augmentation\n",
    "        image = self.quantum_aug(image)  # Apply quantum rotation augmentation\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Initialize Quantum Data Augmentation\n",
    "quantum_aug = QuantumDataAugmentation(n_qubits=8)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "original_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "original_test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Filter the datasets for labels 3 and 6\n",
    "filtered_train_dataset = FilteredMNIST(original_train_dataset, labels_to_include=[3, 6], quantum_aug=quantum_aug)\n",
    "filtered_test_dataset = FilteredMNIST(original_test_dataset, labels_to_include=[3, 6], quantum_aug=quantum_aug)\n",
    "\n",
    "# Create DataLoaders for the filtered datasets\n",
    "train_loader = DataLoader(filtered_train_dataset, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(filtered_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "###############\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "for epoch in range(6):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    evaluate(model, device, test_loader)\n",
    "torch.save(model.state_dict(), 'model_weights_KAN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
